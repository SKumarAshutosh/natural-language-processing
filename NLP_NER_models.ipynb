{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOH+RgXDMM9nQnQ1whDzr7z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SKumarAshutosh/natural-language-processing/blob/master/NLP_NER_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qGtOdRTSjYJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER).\n",
        "\n",
        "### 1. What is NER?\n",
        "\n",
        "Named Entity Recognition (NER) is a sub-task of information extraction that classifies named entities in text into predefined categories such as names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
        "\n",
        "Example:\n",
        "\n",
        "Input: \"Apple is planning to buy U.K. startup for $1 billion.\"\n",
        "\n",
        "Output:\n",
        "- \"Apple\" → ORGANIZATION\n",
        "- \"U.K.\" → LOCATION\n",
        "- \"$1 billion\" → MONEY\n",
        "\n",
        "### 2. When is it required?\n",
        "\n",
        "NER is required in various scenarios including:\n",
        "\n",
        "- Information retrieval: Enhance search by tagging named entities in documents.\n",
        "- Content recommendation: Based on entities identified in the content.\n",
        "- Data analytics: Extract structured information from unstructured data sources.\n",
        "- Knowledge graph construction: Identifying entities and their relations.\n",
        "- Automating business processes: e.g., extracting financial information or client names from emails.\n",
        "\n",
        "### 3. Why is it required?\n",
        "\n",
        "- **Structure to Unstructured Data:** Most of the world's data is unstructured. NER provides a way to extract structured information.\n",
        "- **Efficiency:** It's an automated way to process large datasets and extract valuable information.\n",
        "- **Enhanced Analysis:** By identifying entities, data scientists and analysts can focus on more specific aspects of data analysis.\n",
        "\n",
        "### 4. How can we do NER?\n",
        "\n",
        "NER can be achieved through:\n",
        "\n",
        "- **Rule-based Systems:** Using regular expressions and dictionaries to identify named entities.\n",
        "- **Statistical Models:** Such as Conditional Random Fields (CRF).\n",
        "- **Deep Learning:** Using architectures like Recurrent Neural Networks (RNNs) or Transformer-based models like BERT.\n",
        "\n",
        "### 5. Different types of models available for NER:\n",
        "\n",
        "1. **Rule-based Models:**\n",
        "    - Pros: Highly specific, can be very accurate for known patterns.\n",
        "    - Cons: Not adaptive; need manual effort for rule creation.\n",
        "\n",
        "2. **Statistical Models:**\n",
        "   - **Hidden Markov Models (HMM):** Uses states and transitions with probabilities.\n",
        "   - **Conditional Random Fields (CRF):** Especially popular for sequence labeling tasks.\n",
        "     - Pros: Take into account the context.\n",
        "     - Cons: Require feature engineering; might be outperformed by deep models.\n",
        "\n",
        "3. **Deep Learning Models:**\n",
        "   - **RNNs (LSTM/GRU):** Effective for sequence labeling; consider sequence context.\n",
        "   - **Bidirectional LSTMs:** Capture forward and backward context.\n",
        "   - **Transformer-based Models (like BERT, RoBERTa, etc.):** Pre-trained on vast corpora and fine-tuned for NER.\n",
        "     - Pros: State-of-the-art performance, captures deep contextual information.\n",
        "     - Cons: Computationally intensive.\n",
        "\n",
        "4. **Hybrid Models:**\n",
        "   - Combine rule-based, statistical, and deep learning methods to benefit from each.\n",
        "\n",
        "5. **Ensemble Models:**\n",
        "   - Combine predictions from multiple models to achieve better accuracy.\n",
        "\n",
        "6. **Pre-trained Language Models for Transfer Learning:**\n",
        "   - Models like BERT, GPT-2, and others can be fine-tuned on specific NER tasks to leverage the knowledge they've acquired during their extensive pre-training.\n",
        "\n",
        "### Final Note:\n",
        "\n",
        "The best model often depends on the specific task, the amount and quality of training data, and computational resources. In many real-world applications, a combination of different approaches is used to achieve robust and accurate NER."
      ],
      "metadata": {
        "id": "58qVplyCWPa1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UChcC-COWSZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Layman's perspective.\n",
        "\n",
        "### What is NER?\n",
        "\n",
        "Named Entity Recognition (NER) is like highlighting the names of important things in a story or article. If you read a news article, NER will help underline names of people, places, companies, dates, and more.\n",
        "\n",
        "### Why and When is it Used?\n",
        "\n",
        "Imagine you're quickly skimming through a newspaper and just want to know the main people or places mentioned. NER helps with this. Businesses use it to quickly understand documents, researchers use it to summarize content, and search engines use it to categorize information.\n",
        "\n",
        "### Is NER a Regression or Classification Problem?\n",
        "\n",
        "NER is a classification problem. Think of it like sorting candies by their colors. Each word (or candy) is given a label (or color).\n",
        "\n",
        "### Different Models for NER:\n",
        "\n",
        "1. **Rule-based Models:** Like using a grammar book. If a rule says a word is a name, it's a name.\n",
        "   - Pros: Simple and straightforward.\n",
        "   - Cons: Can't adapt to new patterns easily.\n",
        "\n",
        "2. **Statistical Models:** Imagine asking many friends who often read news about which words are names of companies. Over time, you'd get a good list.\n",
        "   - Example: Conditional Random Fields (CRF) is a popular method here.\n",
        "\n",
        "3. **Deep Learning Models:** It's like a very observant person reading thousands of books and noting down names of people, places, and more. Over time, this person gets really good at spotting names.\n",
        "   - **RNNs:** Think of someone reading a sentence and remembering the previous words to guess the next word's type.\n",
        "   - **Bidirectional LSTMs:** The same as above, but they remember both previous and upcoming words.\n",
        "   - **Transformer-based Models (like BERT):** Think of a genius who's read millions of pages and can instantly tell you what each word in a new sentence likely represents.\n",
        "\n",
        "4. **Hybrid Models:** Combining rules, statistics, and deep learning. Like having a grammar book, friends' suggestions, and a keen observer together.\n",
        "\n",
        "5. **Ensemble Models:** Asking multiple people (or models) about names in a sentence and then going with the majority vote.\n",
        "\n",
        "6. **Pre-trained Models:** Imagine someone who's already an expert in recognizing names in English sentences now fine-tuning their skill to recognize names in science articles. That's how models like BERT are used for NER after being pre-trained on lots of general data.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "NER helps spot names of important things in text. There are many ways to do it, from simple rules to complex deep learning models. The best method often depends on how much data you have and what you want to achieve.\n"
      ],
      "metadata": {
        "id": "rjMh9WxEWiJP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L1moe6eZWkWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Different NER models based on ML & DL teqniques.\n",
        "\n",
        "Named Entity Recognition (NER) is a popular task in Natural Language Processing, and over the years, various machine learning and deep learning models have been proposed and used for it. Here's a list:\n",
        "\n",
        "### Machine Learning Models:\n",
        "\n",
        "1. **Rule-based Systems:** These systems use hand-crafted rules (often regular expressions) to identify entities.\n",
        "  \n",
        "2. **Decision Trees:** They predict the entity label based on features derived from the input text, though they're not the most popular choice for NER.\n",
        "\n",
        "3. **Hidden Markov Models (HMMs):** These consider the sequence of words and their associated states to predict entities.\n",
        "\n",
        "4. **Maximum Entropy Markov Models (MEMMs):** Like HMMs but more flexible, allowing for the inclusion of arbitrary features.\n",
        "\n",
        "5. **Conditional Random Fields (CRFs):** A popular choice for NER in the pre-deep learning era. CRFs consider the entire sequence of words (and their context) to predict entity labels. They can include diverse features, like the word's position in a sentence, its capitalization pattern, and more.\n",
        "\n",
        "### Deep Learning Models:\n",
        "\n",
        "1. **Recurrent Neural Networks (RNNs):** They process sequences word-by-word, maintaining a hidden state from previous words to inform predictions for the current word.\n",
        "\n",
        "   - **Long Short-Term Memory (LSTM):** A type of RNN that's better at capturing long-range dependencies in the data.\n",
        "   \n",
        "   - **Bidirectional LSTMs (BiLSTM):** These process the sequence from both directions (start-to-end and end-to-start), providing a more comprehensive view of the context for each word.\n",
        "\n",
        "2. **Gated Recurrent Units (GRUs):** A variation of RNNs that's simpler than LSTMs but offers similar performance for many tasks.\n",
        "\n",
        "3. **Convolutional Neural Networks (CNNs):** While mostly used for image processing, they've also been employed for NER, capturing local patterns within the text.\n",
        "\n",
        "4. **Transformer-based Models:** These models use self-attention mechanisms to weigh the importance of different words in the sequence relative to a given word.\n",
        "\n",
        "   - **BERT (Bidirectional Encoder Representations from Transformers):** Pre-trained on vast amounts of text and can be fine-tuned for NER.\n",
        "   \n",
        "   - **RoBERTa, DistilBERT, ALBERT:** Variations and optimizations of the original BERT architecture.\n",
        "   \n",
        "   - **XLNet:** A generalized autoregressive model that outperformed BERT on several benchmarks.\n",
        "   \n",
        "   - **GPT (Generative Pre-trained Transformer):** While it's primarily used for generation tasks, with the right setup, it can be adapted for NER.\n",
        "\n",
        "5. **CRF layer on top of Deep Learning models:** It's common to combine the strengths of CRFs and deep learning by using, for example, a BiLSTM to extract features from sequences, followed by a CRF layer to make the final predictions, considering the sequence's structure.\n",
        "\n",
        "### Hybrid Models:\n",
        "\n",
        "These combine elements from both traditional machine learning and deep learning. For instance, using rule-based entity recognition to guide or correct a deep learning model's predictions.\n",
        "\n",
        "In practice, while traditional machine learning models like CRFs were once the state of the art for NER, deep learning models, particularly transformer-based architectures like BERT, currently dominate the field in terms of performance.\n"
      ],
      "metadata": {
        "id": "S-neOWzqXJTn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1o9BSS8-XJAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}