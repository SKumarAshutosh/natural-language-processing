{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh6ed66Of0PWOU1d6UcYsp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SKumarAshutosh/natural-language-processing/blob/master/NLP_NER_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qGtOdRTSjYJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER).\n",
        "\n",
        "### 1. What is NER?\n",
        "\n",
        "Named Entity Recognition (NER) is a sub-task of information extraction that classifies named entities in text into predefined categories such as names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
        "\n",
        "Example:\n",
        "\n",
        "Input: \"Apple is planning to buy U.K. startup for $1 billion.\"\n",
        "\n",
        "Output:\n",
        "- \"Apple\" → ORGANIZATION\n",
        "- \"U.K.\" → LOCATION\n",
        "- \"$1 billion\" → MONEY\n",
        "\n",
        "### 2. When is it required?\n",
        "\n",
        "NER is required in various scenarios including:\n",
        "\n",
        "- Information retrieval: Enhance search by tagging named entities in documents.\n",
        "- Content recommendation: Based on entities identified in the content.\n",
        "- Data analytics: Extract structured information from unstructured data sources.\n",
        "- Knowledge graph construction: Identifying entities and their relations.\n",
        "- Automating business processes: e.g., extracting financial information or client names from emails.\n",
        "\n",
        "### 3. Why is it required?\n",
        "\n",
        "- **Structure to Unstructured Data:** Most of the world's data is unstructured. NER provides a way to extract structured information.\n",
        "- **Efficiency:** It's an automated way to process large datasets and extract valuable information.\n",
        "- **Enhanced Analysis:** By identifying entities, data scientists and analysts can focus on more specific aspects of data analysis.\n",
        "\n",
        "### 4. How can we do NER?\n",
        "\n",
        "NER can be achieved through:\n",
        "\n",
        "- **Rule-based Systems:** Using regular expressions and dictionaries to identify named entities.\n",
        "- **Statistical Models:** Such as Conditional Random Fields (CRF).\n",
        "- **Deep Learning:** Using architectures like Recurrent Neural Networks (RNNs) or Transformer-based models like BERT.\n",
        "\n",
        "### 5. Different types of models available for NER:\n",
        "\n",
        "1. **Rule-based Models:**\n",
        "    - Pros: Highly specific, can be very accurate for known patterns.\n",
        "    - Cons: Not adaptive; need manual effort for rule creation.\n",
        "\n",
        "2. **Statistical Models:**\n",
        "   - **Hidden Markov Models (HMM):** Uses states and transitions with probabilities.\n",
        "   - **Conditional Random Fields (CRF):** Especially popular for sequence labeling tasks.\n",
        "     - Pros: Take into account the context.\n",
        "     - Cons: Require feature engineering; might be outperformed by deep models.\n",
        "\n",
        "3. **Deep Learning Models:**\n",
        "   - **RNNs (LSTM/GRU):** Effective for sequence labeling; consider sequence context.\n",
        "   - **Bidirectional LSTMs:** Capture forward and backward context.\n",
        "   - **Transformer-based Models (like BERT, RoBERTa, etc.):** Pre-trained on vast corpora and fine-tuned for NER.\n",
        "     - Pros: State-of-the-art performance, captures deep contextual information.\n",
        "     - Cons: Computationally intensive.\n",
        "\n",
        "4. **Hybrid Models:**\n",
        "   - Combine rule-based, statistical, and deep learning methods to benefit from each.\n",
        "\n",
        "5. **Ensemble Models:**\n",
        "   - Combine predictions from multiple models to achieve better accuracy.\n",
        "\n",
        "6. **Pre-trained Language Models for Transfer Learning:**\n",
        "   - Models like BERT, GPT-2, and others can be fine-tuned on specific NER tasks to leverage the knowledge they've acquired during their extensive pre-training.\n",
        "\n",
        "### Final Note:\n",
        "\n",
        "The best model often depends on the specific task, the amount and quality of training data, and computational resources. In many real-world applications, a combination of different approaches is used to achieve robust and accurate NER."
      ],
      "metadata": {
        "id": "58qVplyCWPa1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UChcC-COWSZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Layman's perspective.\n",
        "\n",
        "### What is NER?\n",
        "\n",
        "Named Entity Recognition (NER) is like highlighting the names of important things in a story or article. If you read a news article, NER will help underline names of people, places, companies, dates, and more.\n",
        "\n",
        "### Why and When is it Used?\n",
        "\n",
        "Imagine you're quickly skimming through a newspaper and just want to know the main people or places mentioned. NER helps with this. Businesses use it to quickly understand documents, researchers use it to summarize content, and search engines use it to categorize information.\n",
        "\n",
        "### Is NER a Regression or Classification Problem?\n",
        "\n",
        "NER is a classification problem. Think of it like sorting candies by their colors. Each word (or candy) is given a label (or color).\n",
        "\n",
        "### Different Models for NER:\n",
        "\n",
        "1. **Rule-based Models:** Like using a grammar book. If a rule says a word is a name, it's a name.\n",
        "   - Pros: Simple and straightforward.\n",
        "   - Cons: Can't adapt to new patterns easily.\n",
        "\n",
        "2. **Statistical Models:** Imagine asking many friends who often read news about which words are names of companies. Over time, you'd get a good list.\n",
        "   - Example: Conditional Random Fields (CRF) is a popular method here.\n",
        "\n",
        "3. **Deep Learning Models:** It's like a very observant person reading thousands of books and noting down names of people, places, and more. Over time, this person gets really good at spotting names.\n",
        "   - **RNNs:** Think of someone reading a sentence and remembering the previous words to guess the next word's type.\n",
        "   - **Bidirectional LSTMs:** The same as above, but they remember both previous and upcoming words.\n",
        "   - **Transformer-based Models (like BERT):** Think of a genius who's read millions of pages and can instantly tell you what each word in a new sentence likely represents.\n",
        "\n",
        "4. **Hybrid Models:** Combining rules, statistics, and deep learning. Like having a grammar book, friends' suggestions, and a keen observer together.\n",
        "\n",
        "5. **Ensemble Models:** Asking multiple people (or models) about names in a sentence and then going with the majority vote.\n",
        "\n",
        "6. **Pre-trained Models:** Imagine someone who's already an expert in recognizing names in English sentences now fine-tuning their skill to recognize names in science articles. That's how models like BERT are used for NER after being pre-trained on lots of general data.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "NER helps spot names of important things in text. There are many ways to do it, from simple rules to complex deep learning models. The best method often depends on how much data you have and what you want to achieve.\n"
      ],
      "metadata": {
        "id": "rjMh9WxEWiJP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L1moe6eZWkWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Different NER models based on ML & DL teqniques.\n",
        "\n",
        "Named Entity Recognition (NER) is a popular task in Natural Language Processing, and over the years, various machine learning and deep learning models have been proposed and used for it. Here's a list:\n",
        "\n",
        "### Machine Learning Models:\n",
        "\n",
        "1. **Rule-based Systems:** These systems use hand-crafted rules (often regular expressions) to identify entities.\n",
        "  \n",
        "2. **Decision Trees:** They predict the entity label based on features derived from the input text, though they're not the most popular choice for NER.\n",
        "\n",
        "3. **Hidden Markov Models (HMMs):** These consider the sequence of words and their associated states to predict entities.\n",
        "\n",
        "4. **Maximum Entropy Markov Models (MEMMs):** Like HMMs but more flexible, allowing for the inclusion of arbitrary features.\n",
        "\n",
        "5. **Conditional Random Fields (CRFs):** A popular choice for NER in the pre-deep learning era. CRFs consider the entire sequence of words (and their context) to predict entity labels. They can include diverse features, like the word's position in a sentence, its capitalization pattern, and more.\n",
        "\n",
        "### Deep Learning Models:\n",
        "\n",
        "1. **Recurrent Neural Networks (RNNs):** They process sequences word-by-word, maintaining a hidden state from previous words to inform predictions for the current word.\n",
        "\n",
        "   - **Long Short-Term Memory (LSTM):** A type of RNN that's better at capturing long-range dependencies in the data.\n",
        "   \n",
        "   - **Bidirectional LSTMs (BiLSTM):** These process the sequence from both directions (start-to-end and end-to-start), providing a more comprehensive view of the context for each word.\n",
        "\n",
        "2. **Gated Recurrent Units (GRUs):** A variation of RNNs that's simpler than LSTMs but offers similar performance for many tasks.\n",
        "\n",
        "3. **Convolutional Neural Networks (CNNs):** While mostly used for image processing, they've also been employed for NER, capturing local patterns within the text.\n",
        "\n",
        "4. **Transformer-based Models:** These models use self-attention mechanisms to weigh the importance of different words in the sequence relative to a given word.\n",
        "\n",
        "   - **BERT (Bidirectional Encoder Representations from Transformers):** Pre-trained on vast amounts of text and can be fine-tuned for NER.\n",
        "   \n",
        "   - **RoBERTa, DistilBERT, ALBERT:** Variations and optimizations of the original BERT architecture.\n",
        "   \n",
        "   - **XLNet:** A generalized autoregressive model that outperformed BERT on several benchmarks.\n",
        "   \n",
        "   - **GPT (Generative Pre-trained Transformer):** While it's primarily used for generation tasks, with the right setup, it can be adapted for NER.\n",
        "\n",
        "5. **CRF layer on top of Deep Learning models:** It's common to combine the strengths of CRFs and deep learning by using, for example, a BiLSTM to extract features from sequences, followed by a CRF layer to make the final predictions, considering the sequence's structure.\n",
        "\n",
        "### Hybrid Models:\n",
        "\n",
        "These combine elements from both traditional machine learning and deep learning. For instance, using rule-based entity recognition to guide or correct a deep learning model's predictions.\n",
        "\n",
        "In practice, while traditional machine learning models like CRFs were once the state of the art for NER, deep learning models, particularly transformer-based architectures like BERT, currently dominate the field in terms of performance.\n"
      ],
      "metadata": {
        "id": "S-neOWzqXJTn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1o9BSS8-XJAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Each of NER model works.\n",
        "\n",
        "### Machine Learning Models:\n",
        "\n",
        "1. **Rule-based Systems:**\n",
        "    - **How they work:** Uses pre-defined rules, typically written as regular expressions, to identify entities in text.\n",
        "    - **Example:** A rule might state that any sequence of digits with a '-' in the middle is a phone number.\n",
        "\n",
        "2. **Decision Trees:**\n",
        "    - **How they work:** These models ask a series of yes/no questions about the data to make decisions. Each question splits the data into subsets until a prediction is made.\n",
        "    - **Example:** Is the word capitalized? If yes, it might be a proper noun.\n",
        "\n",
        "3. **Hidden Markov Models (HMMs):**\n",
        "    - **How they work:** Assumes each word corresponds to a hidden \"state\" (like a part-of-speech or entity label). Transitions between states and the likelihood of a state emitting a particular word are learned from data.\n",
        "    - **Example:** If the current word is a verb, what's the likelihood the next word is a noun?\n",
        "\n",
        "4. **Maximum Entropy Markov Models (MEMMs):**\n",
        "    - **How they work:** Like HMMs, but more flexible. Instead of just considering the current state's probability, they can consider multiple features and their combinations.\n",
        "    - **Example:** What's the likelihood of a word being a city name if it's capitalized and follows the word \"in\"?\n",
        "\n",
        "5. **Conditional Random Fields (CRFs):**\n",
        "    - **How they work:** They're sequence models like HMMs and MEMMs but take the entire sequence into account when making a prediction for each word.\n",
        "    - **Example:** Given the surrounding words and their features, what's the most likely entity label for the current word?\n",
        "\n",
        "### Deep Learning Models:\n",
        "\n",
        "1. **Recurrent Neural Networks (RNNs):**\n",
        "    - **How they work:** Processes text word-by-word, remembering some information from previous words to help in understanding the current word.\n",
        "    - **Example:** If you've seen \"New\" and you encounter \"York\", your context from previous words helps you label \"York\" as part of a location.\n",
        "\n",
        "2. **Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs):**\n",
        "    - **How they work:** They're specialized RNNs with mechanisms (called gates) that allow them to remember or forget information over longer sequences, solving the vanishing gradient problem of basic RNNs.\n",
        "    - **Example:** Even if many words appear between \"New\" and \"York\", they can still recognize and label them correctly.\n",
        "\n",
        "3. **Convolutional Neural Networks (CNNs):**\n",
        "    - **How they work:** These apply a series of filters on local parts of the input data. For text, it means looking at small windows of words/phrases and extracting features.\n",
        "    - **Example:** Capturing local patterns like \"San Francisco\" as a likely location.\n",
        "\n",
        "4. **Transformer-based Models (e.g., BERT, RoBERTa):**\n",
        "    - **How they work:** Uses self-attention mechanisms to weigh the importance of different words when considering a particular word in a sequence.\n",
        "    - **Example:** If the sentence mentions several companies and then says \"its CEO,\" the model gives more weight to the company names to determine which company \"its\" refers to.\n",
        "\n",
        "5. **CRF layer on top of Deep Learning models:**\n",
        "    - **How they work:** Deep models (like BiLSTMs) extract features, and a CRF layer uses these features to predict the best entity labels for the sequence, taking into account the structure and dependencies in the sequence.\n",
        "    - **Example:** Even if a deep model thinks \"Apple\" in \"Apple's new product\" is a fruit with high probability, the CRF layer can correct it to a company based on context.\n",
        "\n",
        "### Hybrid Models:\n",
        "\n",
        "- **How they work:** These combine rule-based or traditional machine learning techniques with deep learning. For instance, initial predictions might be made using deep learning, but then refined using rule-based logic.\n",
        "- **Example:** A deep learning model might recognize \"Street\" as a location, but a rule-based system can refine this to say if \"Street\" follows a number, it's more likely a street address.\n",
        "\n",
        "Each model's actual operation can get quite technical, involving mathematical equations and algorithms. The above explanations provide a high-level, intuitive view to help understand their basic functioning."
      ],
      "metadata": {
        "id": "PeXGCrofZOfc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IK1V26i9ZT0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End-to-end NER model developement.\n",
        "\n",
        "Creating an end-to-end NER model involves several steps, from data acquisition and preprocessing to model training and evaluation. Here's a general guide, first for a machine learning approach and then for a deep learning one:\n",
        "\n",
        "## 1. Machine Learning Approach (using CRFs):\n",
        "\n",
        "### Data Collection and Preprocessing:\n",
        "1. **Data Collection:** Obtain labeled data where each word/token in your text is annotated with its respective entity type.\n",
        "2. **Tokenization:** Split your text into words/tokens.\n",
        "3. **Feature Engineering:** For NER using CRFs, you'll typically extract various features from each word, such as:\n",
        "   - Word identity\n",
        "   - Word suffix/prefix\n",
        "   - Word shape (capitalization, digit, punctuation patterns)\n",
        "   - Part-of-speech tag\n",
        "   - Surrounding word information\n",
        "\n",
        "### Model Training:\n",
        "1. Use a library like `sklearn-crfsuite` or `CRF++` to train the CRF model on your features and labels.\n",
        "\n",
        "### Evaluation:\n",
        "1. Split your data into a training set and a test set.\n",
        "2. Train your model on the training set and evaluate its performance on the test set using metrics like precision, recall, and F1-score.\n",
        "\n",
        "## 2. Deep Learning Approach (using BiLSTM-CRF):\n",
        "\n",
        "### Data Collection and Preprocessing:\n",
        "1. **Data Collection:** Same as above.\n",
        "2. **Tokenization:** Split your text into words/tokens.\n",
        "3. **Word Embeddings:** Convert words into vectors using embeddings like Word2Vec, GloVe, or FastText.\n",
        "\n",
        "### Model Training:\n",
        "1. Use a deep learning framework like TensorFlow or PyTorch.\n",
        "2. Create a neural network architecture:\n",
        "   - **Embedding Layer:** Convert words into dense vectors.\n",
        "   - **BiLSTM Layer:** Capture context from both forward and backward directions in the text.\n",
        "   - **CRF Layer:** Make sequence predictions taking into account the structure of the output labels.\n",
        "3. Train your model using the training data.\n",
        "\n",
        "### Evaluation:\n",
        "1. Split your data as in the ML approach.\n",
        "2. Evaluate using the same metrics (precision, recall, F1-score).\n",
        "\n",
        "## Implementation Steps:\n",
        "\n",
        "1. **Data Loading:**\n",
        "   ```python\n",
        "   # Example data format: list of sentences where each sentence is a list of (token, label) tuples.\n",
        "   data = [[('Apple', 'B-ORG'), ('is', 'O'), ('a', 'O'), ('company', 'O')], ...]\n",
        "   ```\n",
        "\n",
        "2. **Data Preprocessing:**\n",
        "   - For ML: Extract features and prepare them in the format suitable for the CRF library.\n",
        "   - For DL: Tokenize words and convert them to indices; use embeddings.\n",
        "\n",
        "3. **Model Definition:**\n",
        "   - For ML: Define a CRF model and the set of possible labels.\n",
        "   - For DL: Define the neural network architecture (Embedding -> BiLSTM -> CRF).\n",
        "\n",
        "4. **Training:**\n",
        "   - Train the model using the training dataset.\n",
        "  \n",
        "5. **Evaluation:**\n",
        "   - Predict entity labels on the test set.\n",
        "   - Calculate precision, recall, and F1-score.\n",
        "\n",
        "6. **Deployment (Optional):**\n",
        "   - Once satisfied with the model, you can deploy it as a service using tools like Flask for real-time NER predictions.\n",
        "\n",
        "Remember, the quality of an NER model significantly depends on the quality and quantity of the training data. If possible, gather a diverse and comprehensive labeled dataset or consider using pre-trained models and fine-tuning them on your specific dataset."
      ],
      "metadata": {
        "id": "u8Z2GVIdanQz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CCZQpqjea1HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End-to-end NER model development 2nd Part.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Creating an end-to-end NER model involves several steps, including data preparation, feature extraction (more so for ML models), model selection, training, evaluation, and deployment. I'll guide you through a general pipeline for both traditional machine learning (ML) and deep learning techniques:\n",
        "\n",
        "### 1. Data Preparation:\n",
        "\n",
        "#### a. Data Collection:\n",
        "\n",
        "- **Pre-annotated data**: Use datasets like CoNLL, ACE, or others specific to your domain.\n",
        "- **Annotation tools**: If you have raw text, you can use tools like [Doccano](https://doccano.herokuapp.com/) or [BRAT](http://brat.nlplab.org/) to annotate your data.\n",
        "\n",
        "#### b. Data Split:\n",
        "Split your data into at least three sets:\n",
        "- Training set: To train the model.\n",
        "- Validation set: To tune hyperparameters.\n",
        "- Test set: To evaluate model performance.\n",
        "\n",
        "### 2. Feature Extraction (especially crucial for ML models):\n",
        "\n",
        "#### a. Tokenization:\n",
        "Convert sentences into tokens (usually words).\n",
        "\n",
        "#### b. Lexical Features:\n",
        "- Word embeddings (like Word2Vec or FastText).\n",
        "- Part-of-Speech tags.\n",
        "- Word shapes (capitalization, digit patterns, etc.).\n",
        "\n",
        "#### c. Contextual Features:\n",
        "Previous and next words or tags, n-grams, etc.\n",
        "\n",
        "### 3. Model Selection:\n",
        "\n",
        "#### ML Models:\n",
        "\n",
        "- **Conditional Random Fields (CRFs)**:\n",
        "  - Popular for NER in ML.\n",
        "  - Use tools like `CRFsuite` or `sklearn-crfsuite`.\n",
        "\n",
        "#### Deep Learning Models:\n",
        "\n",
        "- **BiLSTM-CRF**:\n",
        "  - Bi-directional Long Short-Term Memory networks combined with a CRF layer on top.\n",
        "  - Frameworks: TensorFlow, PyTorch.\n",
        "  \n",
        "- **Transformers (like BERT, RoBERTa)**:\n",
        "  - These models can be fine-tuned on NER tasks.\n",
        "  - Use libraries like `HuggingFace Transformers`.\n",
        "\n",
        "### 4. Model Training:\n",
        "\n",
        "#### ML Models:\n",
        "\n",
        "- Feed the extracted features and corresponding labels to the model.\n",
        "- Optimize using algorithms like `L-BFGS` for CRFs.\n",
        "\n",
        "#### Deep Learning Models:\n",
        "\n",
        "- Prepare data loaders to feed data in batches.\n",
        "- Fine-tune on your NER data if using pre-trained transformers.\n",
        "- Optimize using gradient-based methods like Adam.\n",
        "\n",
        "### 5. Model Evaluation:\n",
        "\n",
        "- Common metrics: Precision, Recall, F1-score.\n",
        "- Use tools/libraries like `seqeval` for sequence labeling evaluation.\n",
        "\n",
        "### 6. Hyperparameter Tuning:\n",
        "\n",
        "- For ML models: Regularization parameters, state transition features, etc.\n",
        "- For deep models: Learning rate, dropout rate, number of layers/neurons, etc.\n",
        "- Techniques: Grid search, random search, or Bayesian optimization.\n",
        "\n",
        "### 7. Deployment:\n",
        "\n",
        "- Convert your model to a format suitable for serving (e.g., ONNX).\n",
        "- Deploy using tools like Flask (for a web API) or TensorFlow Serving.\n",
        "- Ensure your deployment solution can handle tokenization and any other preprocessing.\n",
        "\n",
        "### 8. Post-Deployment:\n",
        "\n",
        "- Monitor the model for any drifts in performance.\n",
        "- Periodically retrain if you have accumulating new data.\n",
        "\n",
        "### Tips:\n",
        "\n",
        "1. **Transfer Learning**: Especially for deep learning, leverage pre-trained models to benefit from knowledge transfer.\n",
        "2. **Active Learning**: If annotating data from scratch, start with a small set, train your model, then iteratively annotate more challenging examples.\n",
        "3. **Domain Adaptation**: If your target domain differs significantly from your source domain (where you got your initial training data), consider domain adaptation techniques.\n",
        "\n",
        "Remember, building an NER system is iterative. Based on model performance and real-world feedback, you'll likely need to revisit data annotation, feature engineering, and model selection.\n"
      ],
      "metadata": {
        "id": "11TxQ7spa_BW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAKv2RfdbEsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0Emn6siobUKp"
      }
    }
  ]
}