{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEgSFV+xYaMfa/CAlzI/DL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SKumarAshutosh/natural-language-processing/blob/master/NLP_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnpGNhQ4BAhT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA) for NLP (or text mining tasks) involves gaining a deeper understanding of the text data at hand, uncovering patterns, relationships, anomalies, etc. EDA methods for NLP are a combination of both visual and non-visual techniques:\n",
        "\n",
        "### 1. **Basic Statistics**:\n",
        "   - **Document Length Analysis**: Compute the number of words or characters in each document.\n",
        "   - **Vocabulary Analysis**: Check the number of unique words, the most common and least common words.\n",
        "   - **Average Word Length Analysis**: Compute the average length of words.\n",
        "\n",
        "### 2. **Word Frequencies**:\n",
        "   - **Word Frequency Distribution**: Plot the distribution of word frequencies.\n",
        "   - **N-gram Analysis**: Examine common bigrams, trigrams, etc.\n",
        "   - **Stopword Analysis**: Check the frequency of stopwords to decide if removal is necessary.\n",
        "\n",
        "### 3. **Visualization**:\n",
        "   - **Word Cloud**: Visual representation of word frequency.\n",
        "   - **Histograms**: For example, histogram of document lengths.\n",
        "   - **Box plots**: Useful for visualizing summary statistics of document length.\n",
        "\n",
        "### 4. **Tokenization Analysis**:\n",
        "   - Inspect the tokens to ensure that tokenization was effective.\n",
        "\n",
        "### 5. **Part-of-Speech (POS) Tagging**:\n",
        "   - Analyze the distribution of different parts-of-speech in the text.\n",
        "\n",
        "### 6. **Named Entity Recognition (NER)**:\n",
        "   - Identify and categorize named entities. It helps to know how many named entities are there, and what types (e.g., PERSON, ORGANIZATION, LOCATION).\n",
        "\n",
        "### 7. **Sentiment Analysis**:\n",
        "   - Check the distribution of sentiment scores (e.g., positive, negative, neutral).\n",
        "\n",
        "### 8. **Topic Modeling**:\n",
        "   - Use models like Latent Dirichlet Allocation (LDA) to uncover the underlying topics in the text.\n",
        "   - Visualizing topics using tools like pyLDAvis.\n",
        "\n",
        "### 9. **Term Frequency-Inverse Document Frequency (TF-IDF) Analysis**:\n",
        "   - Understand the importance of words or terms in the corpus.\n",
        "\n",
        "### 10. **Concordance Views**:\n",
        "   - Display occurrences of a word with its surrounding context, useful for understanding how words are used.\n",
        "\n",
        "### 11. **Collocation and Co-occurrence**:\n",
        "   - Identify frequently occurring word pairs or groups.\n",
        "\n",
        "### 12. **Correlation Analysis**:\n",
        "   - Understand the relationship between various meta-features, like document length and sentiment score.\n",
        "\n",
        "### 13. **Duplicate Analysis**:\n",
        "   - Check for and analyze any duplicate documents or content.\n",
        "\n",
        "### 14. **Outlier Detection**:\n",
        "   - Identify unusual patterns or anomalies in the text.\n",
        "\n",
        "### 15. **Missing Value Analysis**:\n",
        "   - For datasets with structured attributes along with text, analyze missing patterns.\n",
        "\n",
        "### 16. **Time Series Analysis** (if applicable):\n",
        "   - When the data has a temporal component, check trends, seasonality, etc., in the text generation.\n",
        "\n",
        "### 17. **Embedding Visualization**:\n",
        "   - If you're using embeddings (like Word2Vec, GloVe), you can visualize them using dimensionality reduction techniques like t-SNE or UMAP.\n",
        "\n",
        "Remember, the goal of EDA in NLP is not only to understand the data but also to determine the preprocessing steps, feature engineering techniques, and potential models to be applied. The specific EDA methods you should apply will depend on your text data and the problem you aim to solve."
      ],
      "metadata": {
        "id": "HNhQUc1jBC33"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "14krcrAQBDmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Let's delve deeper into the application scenarios for each EDA method in NLP:\n",
        "\n",
        "### 1. **Basic Statistics**:\n",
        "\n",
        "- **When & Where**: Applied in almost all NLP tasks to gain a high-level understanding of the dataset.\n",
        "  \n",
        "- **How & Use Case**:\n",
        "  - **Document Length Analysis**: Helps in identifying outliers or exceptionally long/short texts. Useful for tasks like document classification where length might affect model performance.\n",
        "  - **Vocabulary Analysis**: Useful to determine the diversity of words, which can be critical in tasks like machine translation or text summarization.\n",
        "  - **Average Word Length Analysis**: Aids in understanding the complexity of texts. For example, in assessing the readability level of texts.\n",
        "\n",
        "### 2. **Word Frequencies**:\n",
        "\n",
        "- **When & Where**: Useful when understanding which words dominate a text corpus or when cleaning and preprocessing data.\n",
        "  \n",
        "- **How & Use Case**:\n",
        "  - **Word Frequency Distribution**: Used in tasks like keyword extraction or to understand theme concentration.\n",
        "  - **N-gram Analysis**: Essential for tasks like autocomplete suggestions or spelling correction.\n",
        "  - **Stopword Analysis**: Particularly helpful when preprocessing data for tasks like topic modeling where common words might be noise.\n",
        "\n",
        "### 3. **Visualization**:\n",
        "\n",
        "- **When & Where**: In almost all initial stages of NLP tasks to get a visual understanding of the data.\n",
        "  \n",
        "- **How & Use Case**:\n",
        "  - **Word Cloud**: Used for presentations or to quickly visualize dominant themes in user reviews.\n",
        "  - **Histograms**: Useful to visualize the distribution of sentence lengths in tasks like sentiment analysis.\n",
        "  - **Box plots**: Applied in research or academic settings to visualize variances in text lengths.\n",
        "\n",
        "### 4. **Tokenization Analysis**:\n",
        "\n",
        "- **When & Where**: Applied post-tokenization to ensure that the tokenization process has been effective.\n",
        "  \n",
        "- **How & Use Case**: Evaluating token outputs, especially in languages or datasets where standard tokenization might not work effectively. Critical in linguistic research or custom language models.\n",
        "\n",
        "### 5. **Part-of-Speech (POS) Tagging**:\n",
        "\n",
        "- **When & Where**: Applied when understanding the grammatical structure of sentences is important.\n",
        "  \n",
        "- **How & Use Case**: Used in tasks like grammar correction tools or to extract specific entities like nouns or verbs for linguistic analyses.\n",
        "\n",
        "### 6. **Named Entity Recognition (NER)**:\n",
        "\n",
        "- **When & Where**: When identifying named entities like names, places, and organizations in text.\n",
        "  \n",
        "- **How & Use Case**: In tasks like automated news categorization or to extract structured information from unstructured texts.\n",
        "\n",
        "### 7. **Sentiment Analysis**:\n",
        "\n",
        "- **When & Where**: Used with customer reviews, feedback, or any subjective text data.\n",
        "  \n",
        "- **How & Use Case**: Businesses use this for brand monitoring or product feedback analysis.\n",
        "\n",
        "### 8. **Topic Modeling**:\n",
        "\n",
        "- **When & Where**: When trying to uncover underlying topics from large volumes of text.\n",
        "  \n",
        "- **How & Use Case**: News agencies might use topic modeling to categorize news articles, or businesses might use it to cluster customer feedback into topics.\n",
        "\n",
        "### 9. **Term Frequency-Inverse Document Frequency (TF-IDF) Analysis**:\n",
        "\n",
        "- **When & Where**: To understand the importance of words in relation to their frequency in documents and the entire corpus.\n",
        "  \n",
        "- **How & Use Case**: Content recommendation engines or search engines often use TF-IDF to rank the importance of content.\n",
        "\n",
        "### 10. **Concordance Views**:\n",
        "\n",
        "- **When & Where**: Linguistic research or detailed text analysis.\n",
        "  \n",
        "- **How & Use Case**: Used by researchers to understand the context and usage patterns of specific words in literature or historical texts.\n",
        "\n",
        "### 11. **Collocation and Co-occurrence**:\n",
        "\n",
        "- **When & Where**: Applied in tasks where understanding word pairing patterns is essential.\n",
        "  \n",
        "- **How & Use Case**: Essential in linguistics research or to develop better language models for chatbots.\n",
        "\n",
        "### 12. **Correlation Analysis**:\n",
        "\n",
        "- **When & Where**: Used with datasets that have multiple features or meta-attributes.\n",
        "  \n",
        "- **How & Use Case**: Used by news agencies to see if article length correlates with readership metrics.\n",
        "\n",
        "### 13. **Duplicate Analysis**:\n",
        "\n",
        "- **When & Where**: Datasets where there might be repeated entries or content.\n",
        "  \n",
        "- **How & Use Case**: Plagiarism detection tools or database cleanup tasks.\n",
        "\n",
        "### 14. **Outlier Detection**:\n",
        "\n",
        "- **When & Where**: Text datasets where anomalies can provide insights or need to be treated differently.\n",
        "  \n",
        "- **How & Use Case**: Fraud detection in user reviews or outlier detection in customer feedback.\n",
        "\n",
        "### 15. **Missing Value Analysis**:\n",
        "\n",
        "- **When & Where**: Datasets with structured attributes accompanying text.\n",
        "  \n",
        "- **How & Use Case**: Data cleanup for databases that store user-generated content with metadata.\n",
        "\n",
        "### 16. **Time Series Analysis**:\n",
        "\n",
        "- **When & Where**: Text data with temporal attributes, like tweets or news articles over time.\n",
        "  \n",
        "- **How & Use Case**: News agencies analyzing the frequency of topics over time or businesses tracking brand mentions.\n",
        "\n",
        "### 17. **Embedding Visualization**:\n",
        "\n",
        "- **When & Where**: Post-embedding generation in tasks that utilize word embeddings.\n",
        "  \n",
        "- **How & Use Case**: Researchers might visualize word embeddings to ensure semantically similar words cluster together.\n",
        "\n",
        "In summary, EDA methods in NLP provide a foundation to better understand, clean, and preprocess text data. The methods you choose will largely depend on the nature of your text data and the problem you're aiming to solve."
      ],
      "metadata": {
        "id": "--obEpahBXVh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHhgmz97BX7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Providing a full-fledged code for each of the mentioned EDA methods would be extensive, but I can definitely provide you with short code snippets or descriptions of how you might perform each in Python, particularly using libraries such as `pandas`, `nltk`, and `matplotlib`. Let's break it down:\n",
        "\n",
        "### 1. Basic Statistics:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming data is in a DataFrame named df and column name is 'text'\n",
        "df['doc_length'] = df['text'].apply(lambda x: len(x.split()))\n",
        "avg_word_length = df['text'].apply(lambda x: sum(len(word) for word in x.split()) / len(x.split()))\n",
        "vocab = set(' '.join(df['text']).split())\n",
        "```\n",
        "\n",
        "### 2. Word Frequencies:\n",
        "\n",
        "```python\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "tokens = ' '.join(df['text']).split()\n",
        "word_freq = Counter(tokens)\n",
        "bigrams = Counter(ngrams(tokens, 2))\n",
        "```\n",
        "\n",
        "### 3. Visualization:\n",
        "\n",
        "For Word Cloud:\n",
        "```python\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordcloud = WordCloud().generate(' '.join(df['text']))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 4. Tokenization Analysis:\n",
        "(Just demonstrating basic tokenization)\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df['tokens'] = df['text'].apply(word_tokenize)\n",
        "```\n",
        "\n",
        "### 5. Part-of-Speech (POS) Tagging:\n",
        "\n",
        "```python\n",
        "from nltk import pos_tag\n",
        "\n",
        "df['POS_tags'] = df['tokens'].apply(pos_tag)\n",
        "```\n",
        "\n",
        "### 6. Named Entity Recognition:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "\n",
        "def named_entities(text):\n",
        "    return [chunk for chunk in nltk.ne_chunk(pos_tag(word_tokenize(text))) if isinstance(chunk, nltk.Tree)]\n",
        "\n",
        "df['named_entities'] = df['text'].apply(named_entities)\n",
        "```\n",
        "\n",
        "### 7. Sentiment Analysis:\n",
        "\n",
        "Using TextBlob for simplicity:\n",
        "```python\n",
        "from textblob import TextBlob\n",
        "\n",
        "df['sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "```\n",
        "\n",
        "### 8. Topic Modeling:\n",
        "\n",
        "Using LDA from Gensim:\n",
        "```python\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "dictionary = corpora.Dictionary(df['tokens'])\n",
        "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
        "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
        "topics = lda_model.print_topics(num_words=4)\n",
        "```\n",
        "\n",
        "### 9. TF-IDF Analysis:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "```\n",
        "\n",
        "### 10. Concordance Views:\n",
        "\n",
        "Using `nltk.text.Text`:\n",
        "```python\n",
        "from nltk.text import Text\n",
        "\n",
        "tokens = word_tokenize(' '.join(df['text']))\n",
        "text_obj = Text(tokens)\n",
        "text_obj.concordance('example_word', lines=5)  # Displays 5 lines of context for 'example_word'\n",
        "```\n",
        "\n",
        "### 11. Collocation and Co-occurrence:\n",
        "\n",
        "```python\n",
        "text_obj = Text(tokens)\n",
        "text_obj.collocations()\n",
        "```\n",
        "\n",
        "### 12. Correlation Analysis:\n",
        "\n",
        "Using `pandas`:\n",
        "```python\n",
        "correlation = df[['doc_length', 'sentiment']].corr()  # Correlation between document length and sentiment\n",
        "```\n",
        "\n",
        "### 13. Duplicate Analysis:\n",
        "\n",
        "```python\n",
        "duplicates = df[df['text'].duplicated()]\n",
        "```\n",
        "\n",
        "### 14. Outlier Detection:\n",
        "\n",
        "(Just a simple method using z-scores):\n",
        "```python\n",
        "from scipy.stats import zscore\n",
        "\n",
        "df['z_score'] = zscore(df['doc_length'])\n",
        "outliers = df[df['z_score'].abs() > 3]\n",
        "```\n",
        "\n",
        "### 15. Missing Value Analysis:\n",
        "\n",
        "```python\n",
        "missing_vals = df.isnull().sum()\n",
        "```\n",
        "\n",
        "### 16. Time Series Analysis:\n",
        "\n",
        "Assuming there's a datetime column named 'date':\n",
        "```python\n",
        "df.set_index('date', inplace=True)\n",
        "df.resample('M').size().plot()  # Monthly frequency\n",
        "```\n",
        "\n",
        "### 17. Embedding Visualization:\n",
        "\n",
        "Using Word2Vec for embeddings and TSNE for visualization:\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "model = Word2Vec(df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "word_vectors = model.wv\n",
        "tsne = TSNE(n_components=2)\n",
        "embed_tsne = tsne.fit_transform(word_vectors.vectors[:300, :])  # Only visualizing the first 300 vectors for brevity\n",
        "\n",
        "plt.scatter(embed_tsne[:, 0], embed_tsne[:, 1])\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Please note that these are just basic implementations and may require additional tuning and processing based on the dataset and use-case. The required packages (`nltk`, `gensim`, `pandas`, `matplotlib`, `textblob`, `wordcloud`, `sklearn`, `scipy`) need to be installed, and some functionalities in the above code might need additional datasets to be downloaded (like `nltk.download('averaged_perceptron_tagger')` for POS tagging)."
      ],
      "metadata": {
        "id": "90tXt8LPCkNr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mp5nvdjwCk2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OTToSu4ZCsJS"
      }
    }
  ]
}