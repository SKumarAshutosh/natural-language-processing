{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNG8lyvaiQCKf79pl3xGU3G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SKumarAshutosh/natural-language-processing/blob/master/Preprocessing_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-9oxnS3-YnQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Processing (NLP) involves a series of steps to convert raw text into a format that is understandable by machine learning models. Here is a general guide on common NLP preprocessing steps:\n",
        "\n",
        "### 1. Text Cleaning:\n",
        "   - **Lowercasing:** Convert all text to lowercase to maintain consistency.\n",
        "   - **Remove Punctuation:** Eliminate punctuation symbols.\n",
        "   - **Handling Special Characters:** Address symbols, emojis, etc.\n",
        "   - **Remove Stopwords:** Exclude common words (e.g., \"and\", \"the\") that may not add much meaning.\n",
        "   - **Remove HTML Tags:** If working with web data, remove any HTML tags.\n",
        "\n",
        "### 2. Tokenization:\n",
        "   - **Word Tokenization:** Break text into words.\n",
        "   - **Sentence Tokenization:** Break text into sentences.\n",
        "\n",
        "### 3. Normalization:\n",
        "   - **Stemming:** Trim words to their root form (e.g., \"running\" to \"run\").\n",
        "   - **Lemmatization:** Reduce words to their base form considering the context (e.g., \"went\" to \"go\").\n",
        "\n",
        "### 4. Handling Numbers:\n",
        "   - **Number Removal:** Sometimes numbers are removed to reduce complexity.\n",
        "   - **Number Replacement:** Replace numbers with tokens or words (e.g., \"1000\" to \"<NUM>\").\n",
        "\n",
        "### 5. Handling URLs and Usernames:\n",
        "   - **URL Removal/Replacement:** Exclude or replace URLs.\n",
        "   - **Username Removal/Replacement:** Exclude or replace usernames or identifiers.\n",
        "\n",
        "### 6. Spell Checking and Correction:\n",
        "   - **Auto-Correction:** Correct misspelled words using predefined dictionaries or algorithms.\n",
        "\n",
        "### 7. Part-of-Speech Tagging:\n",
        "   - Identify and tag the part of speech (e.g., noun, verb) of each word.\n",
        "\n",
        "### 8. Named Entity Recognition:\n",
        "   - Identify and categorize entities (e.g., names, organizations, locations).\n",
        "\n",
        "### 9. Noise Removal:\n",
        "   - **Handle Typos:** Address spelling mistakes.\n",
        "   - **Remove Extra Whitespaces:** Exclude additional space characters.\n",
        "\n",
        "### 10. Text Encoding:\n",
        "   - **One-Hot Encoding:** Convert words into vectors of 0s and 1s.\n",
        "   - **Word Embedding:** Utilize pre-trained models like Word2Vec, GloVe, or use embeddings from models like BERT.\n",
        "\n",
        "### 11. Removing/Handling Contractions:\n",
        "   - **Contraction Expansion:** Convert contractions (e.g., \"it's\" to \"it is\").\n",
        "\n",
        "### 12. Feature Engineering:\n",
        "   - **Bag of Words:** Create a matrix of word frequencies.\n",
        "   - **TF-IDF:** Utilize term frequency-inverse document frequency to reflect words’ importance.\n",
        "\n",
        "### 13. Handling Imbalanced Data:\n",
        "   - **Oversampling:** Duplicate instances from the minority class.\n",
        "   - **Undersampling:** Remove instances from the majority class.\n",
        "\n",
        "### 14. Data Augmentation:\n",
        "   - **Back Translation:** Translate text to another language and then back to the original.\n",
        "   - **Synonym Replacement:** Replace words with their synonyms.\n",
        "\n",
        "### 15. Dependency Parsing:\n",
        "   - Analyze the grammatical structure and determine the dependencies between words.\n",
        "\n",
        "### 16. Coreference Resolution:\n",
        "   - Identify when different words (pronouns, nouns) refer to the same entity in the text.\n",
        "\n",
        "### 17. Sentiment Analysis (if applicable):\n",
        "   - Analyze and classify the sentiment expressed in the text.\n",
        "\n",
        "### 18. Document Classification (if applicable):\n",
        "   - Categorize documents into predefined classes.\n",
        "\n",
        "### 19. Creating Sequences (for deep learning):\n",
        "   - Convert text into sequences of tokens or embeddings for input into models like LSTMs or GRUs.\n",
        "\n",
        "### 20. Padding Sequences (for deep learning):\n",
        "   - Ensure that all text sequences are of the same length for model training.\n",
        "\n",
        "### 21. Creating Data Batches (for deep learning):\n",
        "   - Group data into batches for efficient training.\n",
        "\n",
        "Remember that the relevance of each step can depend on the specific NLP task and data at hand. Always tailor your preprocessing pipeline according to the requirements and characteristics of your problem."
      ],
      "metadata": {
        "id": "IxANM8zbp8uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# or prefix with ! in Jupyter notebook\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "fTxASEBPp9dj",
        "outputId": "1e5d35d3-9546-4a88-96d7-d619e6463ffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-15 18:46:16.441835: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-15 18:46:17.698135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example Text\n",
        "text = \"Hello, I'm learning NLP! Visit https://www.nlp-example.com for more info. It's exciting, isn't it?\"\n",
        "\n",
        "# 1. Text Cleaning\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Lowercasing\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r\"<.*?>\", \"\", text)  # Remove HTML tags if any\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# 2. Tokenization\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# 3. Remove Stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# 4. Lemmatization\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Applying Preprocessing Steps\n",
        "cleaned_text = clean_text(text)\n",
        "tokens = tokenize_text(cleaned_text)\n",
        "tokens_without_stopwords = remove_stopwords(tokens)\n",
        "lemmatized_tokens = lemmatize_tokens(tokens_without_stopwords)\n",
        "\n",
        "# Output\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Cleaned Text: {cleaned_text}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Tokens without Stopwords: {tokens_without_stopwords}\")\n",
        "print(f\"Lemmatized Tokens: {lemmatized_tokens}\")\n"
      ],
      "metadata": {
        "id": "ibeiMDNAqMdc",
        "outputId": "885c3790-b365-487b-9ccb-c427d1619b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello, I'm learning NLP! Visit https://www.nlp-example.com for more info. It's exciting, isn't it?\n",
            "Cleaned Text: hello im learning nlp visit for more info its exciting isnt it\n",
            "Tokens: ['hello', 'im', 'learning', 'nlp', 'visit', 'for', 'more', 'info', 'its', 'exciting', 'isnt', 'it']\n",
            "Tokens without Stopwords: ['hello', 'im', 'learning', 'nlp', 'visit', 'info', 'exciting', 'isnt']\n",
            "Lemmatized Tokens: ['hello', 'im', 'learning', 'nlp', 'visit', 'info', 'exciting', 'isnt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**:\n",
        "clean_text: Lowercases text, removes URLs, HTML tags, punctuation, and extra spaces.\n",
        "\n",
        "\n",
        "**tokenize_text**: Splits the text into words.\n",
        "\n",
        "\n",
        "**remove_stopwords**: Excludes common English words (stopwords) from the tokens.\n",
        "\n",
        "\n",
        "**lemmatize_tokens**: Converts each word to its base or root form.\n",
        "Note:\n",
        "Ensure to have Python and the required packages installed to run this code on your local machine. Remember to choose the preprocessing steps that are relevant to your specific use case and data.\n",
        "\n",
        "Feel free to add or modify steps according to your NLP task, such as using different tokenization/normalization techniques, adding feature engineering steps, etc."
      ],
      "metadata": {
        "id": "8BCEikm2sGiv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9Ej6ETqqal1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the previous guide provides an extensive overview of many common preprocessing steps, there are other techniques and strategies that might be relevant for specific Natural Language Processing (NLP) applications:\n",
        "\n",
        "### 1. Context Preservation Techniques:\n",
        "   - **Coreference Resolution:** Resolve words that refer to the same entity (e.g., “Obama” and “he”).\n",
        "   - **Anaphora Resolution:** Identify what a pronoun or a noun phrase refers to.\n",
        "\n",
        "### 2. Text Summarization:\n",
        "   - **Extractive Summarization:** Selecting important sentences or phrases as they are.\n",
        "   - **Abstractive Summarization:** Generating new sentences that retain the original meaning.\n",
        "\n",
        "### 3. Dealing with Slangs and Abbreviations:\n",
        "   - **Slang Normalization:** Convert slang or informal expressions to their standard form.\n",
        "   - **Abbreviation Expansion:** Convert abbreviations to their full forms.\n",
        "\n",
        "### 4. Language Detection:\n",
        "   - Identifying the language of the text.\n",
        "\n",
        "### 5. Morphological Analysis:\n",
        "   - Understanding the structure and construction of words.\n",
        "\n",
        "### 6. Collocation Extraction:\n",
        "   - Identifying words that often occur together.\n",
        "\n",
        "### 7. Language Translation:\n",
        "   - If dealing with multilingual data, translating text to a consistent language.\n",
        "\n",
        "### 8. Language Model Fine-tuning:\n",
        "   - Adjusting pre-trained language models to better suit specific tasks/domains.\n",
        "\n",
        "### 9. Handling Dates and Times:\n",
        "   - Standardizing date and time formats.\n",
        "\n",
        "### 10. Word Case Normalization:\n",
        "   - Handling capitalized words used for emphasis or in headers.\n",
        "\n",
        "### 11. Building Vocabulary:\n",
        "   - Creating a mapping of words to unique integer indices for tokenization.\n",
        "\n",
        "### 12. Bi-gram or N-gram Models:\n",
        "   - Considering multiple word phrases as features.\n",
        "\n",
        "### 13. Syntactic Parsing:\n",
        "   - Understanding sentence structure and hierarchical organization of words.\n",
        "\n",
        "### 14. Negation Handling:\n",
        "   - Recognizing and handling negated expressions.\n",
        "\n",
        "### 15. Code Switching Handling:\n",
        "   - Managing instances where two or more languages are used within the same utterance or context.\n",
        "\n",
        "### 16. Text Alignment:\n",
        "   - Aligning text in parallel corpora (useful in translation tasks).\n",
        "\n",
        "### 17. Dialogue Turn Breaking:\n",
        "   - Separating and identifying different turns in dialogue or conversation data.\n",
        "\n",
        "### 18. Discourse Analysis:\n",
        "   - Understanding and identifying logical structures and argumentation within the text.\n",
        "\n",
        "### 19. Phonetics Normalization:\n",
        "   - Converting words to a representation of their phonetic form.\n",
        "\n",
        "### 20. Text Categorization:\n",
        "   - Assigning predefined labels or categories based on content.\n",
        "\n",
        "### 21. Semantic Role Labeling (SRL):\n",
        "   - Determining the semantic roles of constituent phrases in a sentence.\n",
        "\n",
        "### 22. Identifying Text Genres:\n",
        "   - Identifying the genre of text like news, fiction, scientific paper, etc.\n",
        "\n",
        "### 23. Multi-modal Data Handling:\n",
        "   - Managing data that combines text with other modalities, such as images or audio.\n",
        "\n",
        "### 24. Encoding Techniques:\n",
        "   - Implementing diverse text encoding strategies like BPE (Byte Pair Encoding).\n",
        "\n",
        "### 25. Relevance and Redundancy Check:\n",
        "   - Ensuring the provided text is relevant and not redundant for the task.\n",
        "\n",
        "### 26. Dialect Identification:\n",
        "   - Identifying specific dialects within a language.\n",
        "\n",
        "### 27. Paraphrase Identification:\n",
        "   - Identifying and handling paraphrases in text.\n",
        "\n",
        "### 28. Text Segmentation:\n",
        "   - Dividing text into meaningful segments or chunks.\n",
        "\n",
        "The preprocessing steps should be selected and implemented based on the specific NLP task, dataset characteristics, and desired outcomes. Always tailor your preprocessing strategy according to the unique demands and challenges of your project."
      ],
      "metadata": {
        "id": "OwlT6XvgvddU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "egTDOzhuvjki"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}